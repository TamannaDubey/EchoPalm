# ğŸ§  EchoPlam â€“ AI-Powered Sign Language to Speech Dashboard

**EchoPlam** is a real-time accessibility dashboard that detects sign language gestures using a Convolutional Neural Network (CNN) and translates them into readable text and spoken audio. Designed to assist individuals with speech and hearing impairments, the app combines gesture recognition, emotion detection, vitals simulation, and speech translation into a unified, user-friendly interface.

---

## ğŸ“– Project Description

EchoPlam captures hand gestures via webcam, preprocesses the frames, and classifies them into alphabet signs (Aâ€“Y) using a trained CNN model deployed with TensorFlow Lite. The predicted sign is displayed on-screen and spoken aloud using pyttsx3. To enhance stability, a rolling buffer smooths predictions over time.

The app also includes emotion detection from facial expressions, simulated vitals (heart rate and oxygen level), and a Hindi-to-English speech translation module. Users can select their microphone, view live mic status, and follow a two-step flow â€” â€œStart Listeningâ€ and â€œSubmitâ€ â€” for better control. The translated speech is displayed and spoken aloud, bridging communication between signers and non-signers.

EchoPlam is built with modularity and scalability in mind, making it suitable for classrooms, clinics, and public service environments. It showcases practical AI integration, real-time computer vision, and inclusive design.

---

## ğŸ“Œ Key Features

- âœ‹ Real-time sign language detection (Aâ€“Y)
- ğŸ§  CNN model deployed with TensorFlow Lite
- ğŸ”Š Text-to-speech output using pyttsx3
- ğŸ§ Mic selector with live status (ON/OFF/Not Found)
- ğŸ¤ Hindi-to-English speech translation with two-step flow
- ğŸ˜Š Emotion detection from facial expressions
- â¤ï¸ Simulated vitals: heart rate and oxygen level
- ğŸ”„ Prediction buffer for stable output
- ğŸ§¾ Modular design for future expansion

---

## ğŸ§ª Technical Architecture

### ğŸ” Sign Detection Pipeline
- **Input**: Webcam frames via OpenCV and streamlit-webrtc
- **Preprocessing**: Resize, grayscale, normalize frames
- **Model**: CNN trained on static hand gestures (Aâ€“Y)
- **Deployment**: TensorFlow Lite (.tflite) for fast inference
- **Buffering**: `deque` used to smooth predictions
- **Output**: Text + speech via pyttsx3

### ğŸ—£ï¸ Speech Translation Module
- **Input**: Hindi speech via selected microphone
- **Processing**: SpeechRecognition + Googletrans
- **Output**: Translated English text + spoken audio

### ğŸ˜Š Emotion & Vitals
- **Emotion Detection**: Image-based classifier
- **Vitals Simulation**: Randomized heart rate and oxygen level

---

## ğŸ§° Tech Stack

| Layer         | Tools Used                          |
|---------------|-------------------------------------|
| UI            | Streamlit                           |
| Gesture Input | OpenCV, streamlit-webrtc            |
| Sign Detection| TensorFlow Lite (CNN)               |
| Emotion       | Custom image classifier             |
| Audio Input   | SpeechRecognition, PyAudio          |
| Translation   | Googletrans                         |
| Voice Output  | pyttsx3                             |
| Language      | Python 3.9+                         |

---

---

## âš™ï¸ Installation & Usage

### 1. Clone the repository
```bash
git clone https://github.com/your-username/EchoPlam.git
cd EchoPlam

